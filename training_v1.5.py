#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
HIRA BigData Portal - Solar 10.7B LoRA Training Script
================================================================================
Version: 1.5.0
Author: HIRA AI Team
Last Updated: 2025-12-11
Description: íì‡„ë§ í™˜ê²½ì—ì„œ Solar 10.7B ëª¨ë¸ì„ HIRA ë„ë©”ì¸ì— íŠ¹í™”ì‹œí‚¤ëŠ” LoRA í•™ìŠµ

v1.5 Strategy: v1.0 ì•ˆì •ì„± + v2.0 í•µì‹¬ ìˆ˜ì •ë§Œ ì ìš©
  
  [KEEP from v1.0] - ê²€ì¦ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    - LoRA: r=32, alpha=64, dropout=0.05
    - Batch: 4 Ã— 8 = 32 (ì‹¤íš¨ ë°°ì¹˜)
    - max_length: 512
    - lr_scheduler: cosine
    - weight_decay: 0.01
  
  [APPLY from v2.0] - Critical Fixes Only
    âœ… Labels ë§ˆìŠ¤í‚¹ (Assistant ì‘ë‹µë§Œ í•™ìŠµ) - í•™ìŠµ íš¨ìœ¨ í–¥ìƒ
    âœ… eval_steps = save_steps = 100 - Best model ì •í™•ì„±
    âœ… Best Model ë¡œì§ ìˆ˜ì • - trainer.state.best_model_checkpoint í™œìš©
  
  [REMOVED from v2.0] - ì„±ëŠ¥ ì €í•˜ ìš”ì¸ ì œê±°
    âŒ NEFTune (ë¹„í™œì„±í™”)
    âŒ LoRA rank ì¶•ì†Œ (32 ìœ ì§€)
    âŒ ì‹¤íš¨ ë°°ì¹˜ ì¶•ì†Œ (32 ìœ ì§€)
    âŒ linear scheduler (cosine ìœ ì§€)

Usage:
  python training_v1.5.py                           # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
  python training_v1.5.py --epochs 5 --batch_size 4 # íŒŒë¼ë¯¸í„° ì§€ì •
  python training_v1.5.py --resume outputs/hira_lora_20251211_001  # í•™ìŠµ ì¬ê°œ
================================================================================
"""

# ============================================================================
# [CRITICAL] bitsandbytes ì™„ì „ ì°¨ë‹¨ - ë°˜ë“œì‹œ ìµœìƒë‹¨ì— ìœ„ì¹˜
# CUDA 12.3 í™˜ê²½ì—ì„œ bitsandbytes í˜¸í™˜ì„± ì´ìŠˆ íšŒí”¼
# ============================================================================
import os
import sys

os.environ["BITSANDBYTES_NOWELCOME"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# bitsandbytes ëª¨ë“ˆ ì°¨ë‹¨ (import ì‹œë„ ì‹œ None ë°˜í™˜)
_blocked_modules = [
    'bitsandbytes',
    'bitsandbytes.nn',
    'bitsandbytes.optim', 
    'bitsandbytes.cuda_setup',
    'bitsandbytes.functional',
    'bitsandbytes.autograd',
]
for mod in _blocked_modules:
    sys.modules[mod] = None

# ============================================================================
# Imports
# ============================================================================
import json
import logging
import argparse
import hashlib
import shutil
import random
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import Optional, Dict, Any, List

import torch
import torch.nn as nn
from torch.utils.data import Dataset

# Transformers & PEFT
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    EarlyStoppingCallback,
    set_seed,
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
)

# ============================================================================
# Configuration (v1.5 = v1.0 ê¸°ë³¸ê°’ ìœ ì§€)
# ============================================================================
@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • - v1.0 ê²€ì¦ëœ íŒŒë¼ë¯¸í„° + v2.0 Critical Fix
    
    v1.5 = v1.0 Stable + v2.0 Critical Fixes
    """
    
    # === ê²½ë¡œ ì„¤ì • ===
    base_dir: str = "/home/work/LLM_Solar/opnAI_5.1"
    model_name: str = "model/SOLAR-10.7B-Instruct-v1.0"
    dataset_path: str = "dataset/hira_solar_training_v11_11_final_3_cleaned2.json"
    output_base_dir: str = "outputs"
    
    # === ì‹¤í—˜ ê´€ë¦¬ ===
    experiment_name: Optional[str] = None
    version_prefix: str = "hira_lora"
    resume_from: Optional[str] = None
    
    # === LoRA ì„¤ì • (v1.0 ìœ ì§€) ===
    lora_r: int = 32              # v1.0 ìœ ì§€
    lora_alpha: int = 64          # v1.0 ìœ ì§€
    lora_dropout: float = 0.05    # v1.0 ìœ ì§€
    lora_target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ])
    
    # === í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (v1.0 ìœ ì§€) ===
    num_epochs: int = 3
    batch_size: int = 4           # v1.0 ìœ ì§€
    gradient_accumulation_steps: int = 8  # v1.0 ìœ ì§€ (ì‹¤íš¨ ë°°ì¹˜: 4Ã—8=32)
    learning_rate: float = 2e-4
    weight_decay: float = 0.01    # v1.0 ìœ ì§€
    warmup_ratio: float = 0.1
    max_length: int = 512         # v1.0 ìœ ì§€
    
    # === ìµœì í™” (v1.0 ìœ ì§€) ===
    fp16: bool = True
    gradient_checkpointing: bool = True
    optim: str = "adamw_torch"
    lr_scheduler_type: str = "cosine"  # v1.0 ìœ ì§€
    
    # === ì €ì¥ ë° ë¡œê¹… (v2.0 Critical Fix ì ìš©) ===
    save_steps: int = 100
    eval_steps: int = 100         # v2.0 Fix: save_stepsì™€ ì¼ì¹˜
    logging_steps: int = 10
    save_total_limit: int = 3
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_loss"
    greater_is_better: bool = False
    
    # === DataLoader ===
    dataloader_num_workers: int = 4
    
    # === Early Stopping ===
    early_stopping_patience: int = 5
    early_stopping_threshold: float = 0.001
    
    # === ì¬í˜„ì„± ===
    seed: int = 42
    
    # === ë°ì´í„° ë¶„í•  ===
    train_ratio: float = 0.9
    
    def __post_init__(self):
        """ê²½ë¡œ ì •ê·œí™” ë° ê²€ì¦"""
        self.base_dir = Path(self.base_dir)
        self.model_path = self.base_dir / self.model_name
        self.dataset_full_path = self.base_dir / self.dataset_path
        self.output_base = self.base_dir / self.output_base_dir
        
    def get_output_dir(self) -> Path:
        """ë²„ì „ ê´€ë¦¬ëœ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        today = datetime.now().strftime("%Y%m%d")
        
        if self.experiment_name:
            run_name = f"{self.version_prefix}_{self.experiment_name}_{today}"
        else:
            existing = list(self.output_base.glob(f"{self.version_prefix}_*_{today}_*"))
            run_num = len(existing) + 1
            run_name = f"{self.version_prefix}_{today}_{run_num:03d}"
        
        output_dir = self.output_base / run_name
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def save(self, path: Path):
        """ì„¤ì •ì„ JSONìœ¼ë¡œ ì €ì¥"""
        config_dict = {k: str(v) if isinstance(v, Path) else v 
                       for k, v in asdict(self).items()}
        config_dict["script_version"] = "1.5.0"
        with open(path / "training_config.json", "w", encoding="utf-8") as f:
            json.dump(config_dict, f, ensure_ascii=False, indent=2)
    
    @classmethod
    def load(cls, path: Path) -> "TrainingConfig":
        """JSONì—ì„œ ì„¤ì • ë¡œë“œ"""
        with open(path / "training_config.json", "r", encoding="utf-8") as f:
            config_dict = json.load(f)
        config_dict.pop("script_version", None)
        return cls(**config_dict)


# ============================================================================
# Logging Setup
# ============================================================================
def setup_logging(output_dir: Path) -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    log_dir = output_dir / "logs"
    log_dir.mkdir(exist_ok=True)
    
    logger = logging.getLogger("HIRA_Training")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    formatter = logging.Formatter(
        "[%(asctime)s] %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    file_handler = logging.FileHandler(
        log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
        encoding="utf-8"
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    return logger


# ============================================================================
# Dataset (v2.0 Critical Fix: Labels ë§ˆìŠ¤í‚¹ ì ìš©)
# ============================================================================
class HIRADataset(Dataset):
    """HIRA í•™ìŠµ ë°ì´í„°ì…‹
    
    v1.5 Critical Fix: Assistant ì‘ë‹µ ë¶€ë¶„ë§Œ í•™ìŠµí•˜ë„ë¡ Labels ë§ˆìŠ¤í‚¹ ì ìš©
    """
    
    REQUIRED_FIELDS = ["id", "text"]
    ASSISTANT_MARKER = "### Assistant:"
    
    def __init__(self, data: List[Dict], tokenizer, max_length: int):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item["text"]
        
        # í† í¬ë‚˜ì´ì§•
        encodings = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        input_ids = encodings["input_ids"].squeeze()
        attention_mask = encodings["attention_mask"].squeeze()
        labels = input_ids.clone()
        
        # ============================================================
        # [v1.5 Critical Fix] Assistant ì‘ë‹µë§Œ í•™ìŠµí•˜ë„ë¡ ë§ˆìŠ¤í‚¹
        # System/User í”„ë¡¬í”„íŠ¸ëŠ” loss ê³„ì‚°ì—ì„œ ì œì™¸ (-100)
        # ============================================================
        if self.ASSISTANT_MARKER in text:
            assistant_start_char = text.find(self.ASSISTANT_MARKER) + len(self.ASSISTANT_MARKER)
            prefix_text = text[:assistant_start_char]
            
            prefix_tokens = self.tokenizer(
                prefix_text,
                truncation=True,
                max_length=self.max_length,
                add_special_tokens=True,
                return_tensors="pt"
            )
            prefix_len = prefix_tokens["input_ids"].shape[1]
            
            # prefix ë¶€ë¶„ ë§ˆìŠ¤í‚¹ (loss ê³„ì‚° ì œì™¸)
            labels[:prefix_len] = -100
        
        # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹
        labels[input_ids == self.tokenizer.pad_token_id] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    @classmethod
    def validate_schema(cls, data: List[Dict], logger: logging.Logger) -> bool:
        """ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        if not data:
            logger.error("âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            return False
        
        sample = data[0]
        missing = [f for f in cls.REQUIRED_FIELDS if f not in sample]
        if missing:
            logger.error(f"âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing}")
            return False
        
        empty_texts = sum(1 for d in data if not d.get("text", "").strip())
        if empty_texts > 0:
            logger.warning(f"âš ï¸ ë¹ˆ text í•„ë“œ: {empty_texts}ê±´")
        
        no_assistant = sum(1 for d in data if cls.ASSISTANT_MARKER not in d.get("text", ""))
        if no_assistant > 0:
            logger.warning(f"âš ï¸ Assistant ë§ˆì»¤ ì—†ìŒ: {no_assistant}ê±´")
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ")
        logger.info(f"   - ì´ {len(data)}ê±´")
        logger.info(f"   - í•„ë“œ: {list(sample.keys())}")
        logger.info(f"   - Assistant ë§ˆì»¤ í¬í•¨: {len(data) - no_assistant}ê±´")
        
        return True


def load_and_split_dataset(config: TrainingConfig, logger: logging.Logger):
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• """
    logger.info(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {config.dataset_full_path}")
    
    with open(config.dataset_full_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if not HIRADataset.validate_schema(data, logger):
        raise ValueError("ë°ì´í„°ì…‹ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨")
    
    data_hash = hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()[:8]
    logger.info(f"   - ë°ì´í„° í•´ì‹œ: {data_hash}")
    
    set_seed(config.seed)
    random.seed(config.seed)
    
    split_idx = int(len(data) * config.train_ratio)
    
    shuffled = data.copy()
    random.shuffle(shuffled)
    
    train_data = shuffled[:split_idx]
    eval_data = shuffled[split_idx:]
    
    logger.info(f"   - Train: {len(train_data)}ê±´ ({config.train_ratio*100:.0f}%)")
    logger.info(f"   - Eval: {len(eval_data)}ê±´ ({(1-config.train_ratio)*100:.0f}%)")
    
    return train_data, eval_data


# ============================================================================
# Model Loading
# ============================================================================
def load_model_and_tokenizer(config: TrainingConfig, logger: logging.Logger):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (íì‡„ë§ í™˜ê²½)"""
    
    logger.info(f"ğŸ”§ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {config.model_path}")
    
    logger.info("   [1/3] í† í¬ë‚˜ì´ì € ë¡œë“œ...")
    tokenizer = AutoTokenizer.from_pretrained(
        config.model_path,
        trust_remote_code=True,
        local_files_only=True,
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    logger.info("   [2/3] ëª¨ë¸ ë¡œë“œ...")
    model = AutoModelForCausalLM.from_pretrained(
        config.model_path,
        torch_dtype=torch.float16 if config.fp16 else torch.float32,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    
    if config.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        logger.info("   âœ… Gradient Checkpointing í™œì„±í™”")
    
    logger.info("   [3/3] LoRA ì–´ëŒ‘í„° ì„¤ì •...")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=config.lora_target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"   âœ… LoRA ì ìš© ì™„ë£Œ")
    logger.info(f"   - LoRA r={config.lora_r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}")
    logger.info(f"   - í•™ìŠµ ê°€ëŠ¥: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)")
    logger.info(f"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    
    return model, tokenizer


# ============================================================================
# Custom Callbacks
# ============================================================================
class TrainingProgressCallback(TrainerCallback):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… ì½œë°±"""
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.best_eval_loss = float("inf")
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            metrics = {k: f"{v:.4f}" if isinstance(v, float) else v 
                      for k, v in logs.items() 
                      if k in ["loss", "eval_loss", "learning_rate"]}
            if metrics:
                self.logger.info(f"   Step {state.global_step}: {metrics}")
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and "eval_loss" in metrics:
            eval_loss = metrics["eval_loss"]
            if eval_loss < self.best_eval_loss:
                self.best_eval_loss = eval_loss
                self.logger.info(f"   ğŸŒŸ New Best Eval Loss: {eval_loss:.4f}")


class SaveConfigCallback(TrainerCallback):
    """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ì„¤ì • ì €ì¥"""
    
    def __init__(self, config: TrainingConfig, output_dir: Path):
        self.config = config
        self.output_dir = output_dir
        
    def on_train_end(self, args, state, control, **kwargs):
        summary = {
            "script_version": "1.5.0",
            "total_steps": state.global_step,
            "best_metric": state.best_metric,
            "best_model_checkpoint": state.best_model_checkpoint,
            "epochs_completed": state.epoch,
            "training_completed": datetime.now().isoformat(),
        }
        with open(self.output_dir / "training_summary.json", "w") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)


# ============================================================================
# Trainer Creation
# ============================================================================
def create_trainer(
    model, 
    tokenizer, 
    train_dataset, 
    eval_dataset, 
    config: TrainingConfig,
    output_dir: Path,
    logger: logging.Logger
) -> Trainer:
    """Trainer ìƒì„± (v1.5: v1.0 ì„¤ì • + v2.0 Critical Fix)"""
    
    training_args = TrainingArguments(
        output_dir=str(output_dir / "checkpoints"),
        
        # í•™ìŠµ ì„¤ì • (v1.0 ìœ ì§€)
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        
        # ì˜µí‹°ë§ˆì´ì € (v1.0 ìœ ì§€)
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_ratio=config.warmup_ratio,
        optim=config.optim,
        lr_scheduler_type=config.lr_scheduler_type,  # cosine
        
        # FP16
        fp16=config.fp16,
        
        # ì €ì¥ ë° í‰ê°€ (v2.0 Critical Fix: eval_steps = save_steps)
        evaluation_strategy="steps",
        eval_steps=config.eval_steps,
        save_strategy="steps",
        save_steps=config.save_steps,
        save_total_limit=config.save_total_limit,
        load_best_model_at_end=config.load_best_model_at_end,
        metric_for_best_model=config.metric_for_best_model,
        greater_is_better=config.greater_is_better,
        
        # ë¡œê¹…
        logging_dir=str(output_dir / "logs" / "tensorboard"),
        logging_steps=config.logging_steps,
        report_to=["tensorboard"],
        
        # DataLoader
        dataloader_num_workers=config.dataloader_num_workers,
        dataloader_pin_memory=True,
        
        # ê¸°íƒ€
        seed=config.seed,
        data_seed=config.seed,
        remove_unused_columns=False,
        
        # íì‡„ë§
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[
            TrainingProgressCallback(logger),
            SaveConfigCallback(config, output_dir),
            EarlyStoppingCallback(
                early_stopping_patience=config.early_stopping_patience,
                early_stopping_threshold=config.early_stopping_threshold,
            ),
        ],
    )
    
    return trainer


# ============================================================================
# Main Training Function (v2.0 Critical Fix: Best Model ë¡œì§ ìˆ˜ì •)
# ============================================================================
def train(config: TrainingConfig):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if config.resume_from:
        output_dir = Path(config.resume_from)
        print(f"ğŸ“‚ í•™ìŠµ ì¬ê°œ: {output_dir}")
    else:
        output_dir = config.get_output_dir()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(output_dir)
    
    logger.info("=" * 60)
    logger.info("ğŸš€ HIRA Solar LoRA Training v1.5")
    logger.info("   (v1.0 Stable + v2.0 Critical Fixes)")
    logger.info("=" * 60)
    logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # v1.5 ì„¤ì • ìš”ì•½
    logger.info("-" * 40)
    logger.info("ğŸ“‹ v1.5 Configuration:")
    logger.info("   [v1.0 ìœ ì§€] LoRA: r=32, alpha=64, dropout=0.05")
    logger.info("   [v1.0 ìœ ì§€] Batch: 4 Ã— 8 = 32 (ì‹¤íš¨)")
    logger.info("   [v1.0 ìœ ì§€] max_length: 512, scheduler: cosine")
    logger.info("   [v2.0 Fix] Labels ë§ˆìŠ¤í‚¹ (Assistantë§Œ í•™ìŠµ)")
    logger.info("   [v2.0 Fix] eval_steps = save_steps = 100")
    logger.info("   [v2.0 Fix] Best Model: trainer.state ê¸°ë°˜")
    
    # ì¬í˜„ì„±
    set_seed(config.seed)
    logger.info(f"ğŸ² Seed: {config.seed}")
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"ğŸ–¥ï¸ GPU: {gpu_name} ({gpu_mem:.1f}GB)")
        torch.cuda.empty_cache()
    else:
        logger.warning("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    # ì„¤ì • ì €ì¥
    config.save(output_dir)
    logger.info("ğŸ“ ì„¤ì • ì €ì¥ ì™„ë£Œ")
    
    # ë°ì´í„° ë¡œë“œ
    logger.info("-" * 40)
    train_data, eval_data = load_and_split_dataset(config, logger)
    
    # ëª¨ë¸ ë¡œë“œ
    logger.info("-" * 40)
    model, tokenizer = load_model_and_tokenizer(config, logger)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    logger.info("-" * 40)
    logger.info("ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„...")
    train_dataset = HIRADataset(train_data, tokenizer, config.max_length)
    eval_dataset = HIRADataset(eval_data, tokenizer, config.max_length)
    logger.info(f"   âœ… Train: {len(train_dataset)}ê±´, Eval: {len(eval_dataset)}ê±´")
    
    # Trainer ìƒì„±
    logger.info("-" * 40)
    logger.info("ğŸ”§ Trainer ì„¤ì •...")
    trainer = create_trainer(
        model, tokenizer, train_dataset, eval_dataset,
        config, output_dir, logger
    )
    
    # í•™ìŠµ ì •ë³´ ì¶œë ¥
    effective_batch = config.batch_size * config.gradient_accumulation_steps
    steps_per_epoch = len(train_dataset) // effective_batch
    total_steps = steps_per_epoch * config.num_epochs
    
    logger.info(f"   - Epochs: {config.num_epochs}")
    logger.info(f"   - Batch: {config.batch_size} Ã— {config.gradient_accumulation_steps} = {effective_batch} (ì‹¤íš¨)")
    logger.info(f"   - Steps/Epoch: ~{steps_per_epoch}")
    logger.info(f"   - Total Steps: ~{total_steps}")
    logger.info(f"   - Learning Rate: {config.learning_rate}")
    logger.info(f"   - LR Scheduler: {config.lr_scheduler_type}")
    
    # í•™ìŠµ ì‹œì‘
    logger.info("-" * 40)
    logger.info("ğŸƒ í•™ìŠµ ì‹œì‘!")
    logger.info("-" * 40)
    
    if config.resume_from:
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()
    
    # ============================================================
    # [v1.5 Critical Fix] Best Model ì €ì¥ ë¡œì§ ìˆ˜ì •
    # ============================================================
    logger.info("-" * 40)
    logger.info("ğŸ’¾ ëª¨ë¸ ì €ì¥...")
    
    # 1. Final Model ì €ì¥
    final_model_dir = output_dir / "final_model"
    final_model_dir.mkdir(exist_ok=True)
    model.save_pretrained(final_model_dir)
    tokenizer.save_pretrained(final_model_dir)
    logger.info(f"   âœ… Final Model: {final_model_dir}")
    
    # 2. Best Model ì €ì¥ (trainer.state ê¸°ë°˜)
    best_model_dir = output_dir / "best_model"
    
    if hasattr(trainer.state, 'best_model_checkpoint') and trainer.state.best_model_checkpoint:
        best_ckpt_path = Path(trainer.state.best_model_checkpoint)
        if best_ckpt_path.exists():
            shutil.copytree(best_ckpt_path, best_model_dir, dirs_exist_ok=True)
            logger.info(f"   âœ… Best Model: {best_model_dir}")
            logger.info(f"      (from: {best_ckpt_path.name})")
            logger.info(f"      Best Eval Loss: {trainer.state.best_metric:.4f}")
        else:
            shutil.copytree(final_model_dir, best_model_dir, dirs_exist_ok=True)
            logger.info(f"   âœ… Best Model: {best_model_dir} (= final)")
    else:
        shutil.copytree(final_model_dir, best_model_dir, dirs_exist_ok=True)
        logger.info(f"   âœ… Best Model: {best_model_dir} (= final)")
    
    # 3. í•™ìŠµ ìš”ì•½ ì €ì¥
    final_summary = {
        "script_version": "1.5.0",
        "strategy": "v1.0 Stable + v2.0 Critical Fixes",
        "total_steps": trainer.state.global_step,
        "best_eval_loss": trainer.state.best_metric,
        "best_checkpoint": str(trainer.state.best_model_checkpoint) if trainer.state.best_model_checkpoint else None,
        "epochs_completed": trainer.state.epoch,
        "train_samples": len(train_dataset),
        "eval_samples": len(eval_dataset),
        "effective_batch_size": effective_batch,
        "training_completed": datetime.now().isoformat(),
    }
    with open(output_dir / "training_summary.json", "w") as f:
        json.dump(final_summary, f, indent=2, ensure_ascii=False)
    
    # ì™„ë£Œ
    logger.info("=" * 60)
    logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"ğŸ“‚ ê²°ê³¼: {output_dir}")
    logger.info(f"ğŸ“Š Best Eval Loss: {trainer.state.best_metric:.4f}")
    logger.info("=" * 60)
    
    return output_dir


# ============================================================================
# Argument Parser
# ============================================================================
def parse_args():
    parser = argparse.ArgumentParser(
        description="HIRA Solar LoRA Training v1.5 (Stable + Critical Fixes)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ê²½ë¡œ
    parser.add_argument("--base_dir", type=str, default="/home/work/LLM_Solar/opnAI_5.1",
                        help="ê¸°ë³¸ ì‘ì—… ë””ë ‰í† ë¦¬")
    parser.add_argument("--model_name", type=str, default="model/SOLAR-10.7B-Instruct-v1.0",
                        help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--dataset_path", type=str, default="dataset/hira_solar_training_v11_11_final_3_cleaned2.json",
                        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--output_base_dir", type=str, default="outputs",
                        help="ì¶œë ¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    
    # ì‹¤í—˜ ê´€ë¦¬
    parser.add_argument("--experiment_name", type=str, default=None,
                        help="ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--resume", type=str, default=None,
                        help="í•™ìŠµ ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬")
    
    # LoRA (v1.0 ê¸°ë³¸ê°’)
    parser.add_argument("--lora_r", type=int, default=32, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=64, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    
    # í•™ìŠµ (v1.0 ê¸°ë³¸ê°’)
    parser.add_argument("--epochs", type=int, default=3, help="í•™ìŠµ ì—í­ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=4, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--grad_accum", type=int, default=8, help="Gradient accumulation steps")
    parser.add_argument("--lr", type=float, default=2e-4, help="í•™ìŠµë¥ ")
    parser.add_argument("--max_length", type=int, default=512, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    
    # ê¸°íƒ€
    parser.add_argument("--seed", type=int, default=42, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--no_fp16", action="store_true", help="FP16 ë¹„í™œì„±í™”")
    
    return parser.parse_args()


# ============================================================================
# Entry Point
# ============================================================================
def main():
    args = parse_args()
    
    config = TrainingConfig(
        base_dir=args.base_dir,
        model_name=args.model_name,
        dataset_path=args.dataset_path,
        output_base_dir=args.output_base_dir,
        experiment_name=args.experiment_name,
        resume_from=args.resume,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.lr,
        max_length=args.max_length,
        seed=args.seed,
        fp16=not args.no_fp16,
    )
    
    output_dir = train(config)
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ê²°ê³¼: {output_dir}")


if __name__ == "__main__":
    main()
